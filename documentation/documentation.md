# üöÄ Proyecto ELT con AWS Glue, Amazon S3 y Amazon Redshift

Este proyecto demuestra el desarrollo completo de un flujo **ETL (Extract, Transform, Load)** en la nube utilizando servicios administrados de **AWS**, con el objetivo de practicar **data wrangling, catalogaci√≥n, transformaci√≥n y carga de datos** para anal√≠tica.

La arquitectura integra **Amazon S3** como capa de almacenamiento, **AWS Glue** como motor de catalogaci√≥n, limpieza y transformaci√≥n de datos (ETL/ELT), y **Amazon Redshift** como data warehouse anal√≠tico.

---

## üß© Arquitectura General

La arquitectura sigue un enfoque **ETL**, donde los datos en bruto se extraen de un almac√©n, luego se transforman, permitiendo trazabilidad y flexibilidad en el procesamiento y por √∫ltimo se cargan en un destino.

**Flujo general del proyecto:**

1. **Amazon S3**
   - Almacenamiento de los archivos fuente (`.json`)
   - Datos con valores nulos, duplicados y variaciones de esquema para pruebas de data wrangling

2. **AWS Glue**
   - Crawlers para inferencia de esquemas y creaci√≥n del **Data Catalog**
   - Jobs visuales (sin c√≥digo) para:
     - Eliminaci√≥n de filas nulas
     - Eliminaci√≥n de duplicados
     - Limpieza y estandarizaci√≥n de datos
   - Transformaciones previas antes de la carga anal√≠tica

3. **Amazon Redshift**
   - Recepci√≥n de los datos transformados desde Glue
   - Almacenamiento estructurado para consultas anal√≠ticas
   - Validaci√≥n del esquema final y consistencia de datos

---

## üîÑ Implementaci√≥n End-to-End del Proyecto
En esta secci√≥n se detallan los pasos ejecutados a lo largo de todo el ciclo del proyecto, desde la ingesta de datos hasta su carga final para an√°lisis.

---

### ü™£ Paso A: Configuraci√≥n de Amazon S3 (Buckets y archivos de origen)

En esta etapa se prepara la capa de almacenamiento que actuar√° como fuente de datos del proyecto.

#### üìÇ Bucket de Amazon S3

* Se utiliza un bucket previamente creado para el proyecto.

* Nombre del bucket: ``` project-s3-glue-redshift-brayan ```

#### üìÑ Archivos de datos

* Dentro del bucket se cargan los archivos que ser√°n consumidos por el proceso ETL.

* Para este proyecto se utiliza un archivo de prueba en formato JSON, el cual simula datos de ventas.

Archivo utilizado: ``` sales_test.json ```

Este archivo representa el origen de datos bruto (raw data) y ser√° consumido posteriormente por los servicios de integraci√≥n y transformaci√≥n definidos en las siguientes etapas del proyecto.

---

### üîê Paso B: Creaci√≥n del rol IAM para la ejecuci√≥n del proyecto

En esta etapa se define el rol de seguridad que permitir√° la comunicaci√≥n entre los distintos servicios de AWS utilizados en el proyecto, aplicando el principio de m√≠nimo privilegio.

#### üéØ Objetivo del rol

El rol ser√° utilizado principalmente por AWS Glue, que act√∫a como el servicio orquestador del flujo ETL. Desde Glue se requiere acceso a:

* Amazon S3 (lectura de datos de origen y escritura de resultados)

* Amazon Redshift (carga de datos transformados)

* Cat√°logo de datos y jobs de Glue

Por este motivo, **Glue es el servicio principal (l√≠der)** que asume este rol durante la ejecuci√≥n del proyecto.

#### üßæ Detalles del rol IAM

* Nombre del rol: ``` rol-project-s3-glue-redshift-brayan ```
* Servicio que asume el rol (Trusted Entity): **AWS Glue**

#### üìú Pol√≠ticas de permisos asignadas

Para efectos del proyecto y pruebas, se asignan las siguientes pol√≠ticas administradas por AWS:

* AWSGlueServiceRole

* AWSGlueConsoleFullAccess

* AmazonS3FullAccess

* AmazonRedshiftFullAccess

> ‚ö†Ô∏è Nota:
En un entorno productivo, estas pol√≠ticas deber√≠an ser reemplazadas por pol√≠ticas personalizadas m√°s restrictivas. Para fines de este proyecto de portafolio, se prioriza la simplicidad y la correcta integraci√≥n entre servicios.

---

### üß™ Paso C: AWS Glue ‚Äì Crawler, Data Catalog y Visual ETL

En esta etapa se prepara el entorno de metadatos y transformaci√≥n que permitir√° trabajar los datos almacenados en Amazon S3 de forma estructurada.

#### üìå Consideraci√≥n clave inicial

Los archivos almacenados en Amazon S3 (en este caso archivos JSON) no poseen un esquema relacional expl√≠cito que pueda ser utilizado directamente para procesos ETL.

Por este motivo, es necesario **inferir previamente el esquema de los datos**, para luego poder:

* Catalogarlos

* Transformarlos

* Cargarlos a un destino anal√≠tico (Amazon Redshift)

#### üîç Uso del AWS Glue Crawler

Para resolver lo anterior, se utiliza un AWS Glue Crawler, el cual:

* Analiza los datos en su origen (S3)

* Infiere autom√°ticamente su estructura (columnas, tipos de datos)

* Registra el esquema como metadatos dentro del AWS Glue Data Catalog

> ‚ö†Ô∏è Importante:
La base de datos del Data Catalog NO almacena datos, √∫nicamente metadatos.
Los datos reales permanecen siempre en Amazon S3.

---

#### üóÇÔ∏è Pasos iniciales de configuraci√≥n

##### i) **Creaci√≥n de la base de datos en Glue Data Catalog**

Se crea una base de datos l√≥gica para organizar los metadatos del proyecto.

* Nombre de la base de datos: ``` db_project_s3_glue_redshift ```

##### **ii) Creaci√≥n del Crawler**

Se configura un crawler apuntando al bucket y ruta donde se encuentran los archivos fuente en S3.

* Nombre del crawler: ``` crawgler_project_s3_glue_redshift ```
* Origen de datos: **Amazon S3 (bucket del proyecto)**, ingresamos al bucket **``` project-s3-glue-redshift-brayan ```** y seleccionamos **``` sales_test.json ```**
* Selccionamos IAM role: **Rol creado anteriormente -> ``` rol-project-s3-glue-redshift-brayan ```**
* Base de datos destino: **Base de datos db_project_s3_glue_redshift en el Data Catalog**
* **Next o Siguiente** y le damos a **Create Crawgler**
> üìù Nota importante:
Las tablas dentro del Data Catalog se crean autom√°ticamente al ejecutar el crawler, en base al esquema inferido.

##### **iii) Ejecuci√≥n y validaci√≥n**

* Se ejecuta el crawler

* Se espera a que finalice correctamente

* Se verifica que:

   * La tabla haya sido creada en la base de datos ``` db_project_s3_glue_redshift ```

   * El esquema inferido sea coherente con los datos de origen

---

### üõ†Ô∏è Paso C (Continuaci√≥n): Creaci√≥n del Visual ETL en AWS Glue

Como segundo punto dentro de esta etapa, se procede a la creaci√≥n del Visual ETL Job en AWS Glue Studio, el cual permitir√° dise√±ar el flujo de transformaci√≥n de datos de forma gr√°fica.

---

#### üé® Visual ETL en AWS Glue Studio

Para este proyecto se utiliza Visual ETL, el cual es ideal para:

* Procesos ETL simples a medianos

* Flujos de transformaci√≥n claros y controlados

* Casos donde se busca minimizar el uso de c√≥digo

> üìå Nota t√©cnica:
Para procesos ETL m√°s complejos, con m√∫ltiples dependencias o alta orquestaci√≥n, se recomienda evaluar servicios como Amazon EMR o AWS Step Functions, integrando varios servicios de AWS.

---

#### üß© Fases del Visual ETL

Dentro del Visual ETL, el flujo de trabajo se organiza en tres fases principales, cada una representada por distintos tipos de nodos:

1. **Sources (Or√≠genes)**
Definen desde d√≥nde se leen los datos.

2. **Transforms (Transformaciones)**
Permiten limpiar, enriquecer y modificar los datos.

3. **Targets (Objetivos)**
Definen el destino final de los datos procesados.

üìç Todos los nodos pueden agregarse desde el √≠cono ‚Äú+‚Äù ubicado en el workspace del Visual ETL.

---

#### ****üîå Fase 1: Nodo de Origen (Source)****

**i) AWS Glue Data Catalog**

Se selecciona el nodo **AWS Glue Data Catalog**, el cual permite conectarse directamente a la tabla creada previamente mediante la ejecuci√≥n del crawler.

Este nodo:

* Utiliza los metadatos almacenados en Glue Data Catalog

* Apunta f√≠sicamente a los datos almacenados en Amazon S3

##### ‚ö†Ô∏è Consideraciones importantes para la previsualizaci√≥n de datos

En caso de no poder previsualizar los datos del origen, se deben verificar los siguientes puntos:

1. **Rol de IAM del Job**

   * Ir a Job details del Visual ETL

   * En la secci√≥n IAM Role, seleccionar el rol creado en el **Paso B**: ``` rol-project-s3-glue-redshift-brayan ```

2. **Refrescar el entorno**

   * Si el rol no aparece inmediatamente, basta con refrescar la p√°gina

   * Al hacerlo, el nodo AWS Glue Data Catalog se eliminar√° autom√°ticamente

   * Se debe volver a agregar el nodo despu√©s de seleccionar el rol correcto

3. **Selecci√≥n de base de datos y tabla**

   * En la opci√≥n Database, seleccionar: ``` db_project_s3_glue_redshift ```
   * En la opci√≥n **Table**, seleccionar la tabla creada por el crawler

Una vez completados estos pasos, los datos podr√°n **previsualizarse correctamente** dentro del Visual ETL.


#### ****üîåüîÑ Fase 2: Transformaci√≥n (Data Wrangling)****

Una vez configurado correctamente el origen de datos, se procede a la fase de transformaci√≥n, donde se aplican buenas pr√°cticas de data wrangling para asegurar la calidad, consistencia y compatibilidad de los datos antes de su carga al destino final.

**ii) Eliminaci√≥n de filas completamente nulas**

Como primera transformaci√≥n, se eliminan aquellas filas que contienen valores nulos en todos sus campos, lo cual suele indicar registros inv√°lidos o ruido en el dataset.

* Nodo utilizado: **Remove Null Rows**

* Ubicaci√≥n: secci√≥n **Transform**

* Conexi√≥n: 
   ```sql
   AWS Glue Data Catalog  ‚Üí  Remove Null Rows
   ```
Al previsualizar los datos despu√©s de aplicar este nodo, se observa que las filas completamente nulas ya no aparecen, confirmando que el proceso de limpieza va por buen camino.

**iii) Eliminaci√≥n de filas duplicadas**

Siguiendo las buenas pr√°cticas de calidad de datos, el siguiente paso consiste en eliminar **registros duplicados**, considerando la fila completa como unidad de comparaci√≥n.

* Nodo utilizado: **Drop Duplicates**

* Ubicaci√≥n: secci√≥n **Transform**

* Conexi√≥n:
   ```sql
   Remove Null Rows  ‚Üí  Drop Duplicates
   ```
Luego de aplicar este nodo, la previsualizaci√≥n de datos muestra que los registros duplicados han sido correctamente eliminados.

**iv) Transformaciones con SQL (normalizaci√≥n y columnas calculadas)**

Para realizar transformaciones m√°s flexibles y r√°pidas, se utiliza el nodo SQL Query, el cual permite aplicar l√≥gica de negocio mediante SQL directamente sobre el flujo de datos.

Este nodo se conecta al paso anterior:
   ```sql
   Drop Duplicates  ‚Üí  SQL Query
   ```

Objetivos de este paso:

* Reemplazar valores nulos en columnas espec√≠ficas

* Normalizar datos num√©ricos

* Crear columnas calculadas

Query utilizada:
```sql
SELECT 
    order_id,
    order_date,
    customer,
    product,
    category,
    COALESCE(quantity, 0) AS quantity,
    COALESCE(unit_price, 0) AS unit_price,
    region,
    COALESCE(quantity * unit_price, 0) AS total_sale
FROM myDataSource
ORDER BY order_id ASC;
```
> üìå Nota importante:
**myDataSource** hace referencia al nombre del nodo anterior, es decir, el nodo **Drop Duplicates**, el cual act√∫a como la tabla fuente para esta consulta.

**v) Alineaci√≥n de tipos de datos (Change Schema)**

Este paso es cr√≠tico dentro del proceso ETL, debido que garantiza la compatibilidad entre el esquema del origen transformado en Glue y el esquema definido en la tabla destino en Amazon Redshift.

* Nodo utilizado: **Change Schema**

* Ubicaci√≥n: secci√≥n **Transform**

* Conexi√≥n:
   ```sql
      SQL Query  ‚Üí  Change Schema
   ```
* Columnas convetidas (Tipo de dato original -> Tipo de dato convertido):
   ```sql
      order_date (string) -> order_date (date)
      unit_price (double) -> unit_price (decimal)
      total_sale (double) -> total_sale (decimal)
   ```
**¬øPor qu√© es tan importante este paso?**

Si los tipos de datos no se alinean correctamente:

* Las columnas con tipos incompatibles en Redshift quedar√°n como NULL

* Glue crear√° columnas adicionales con nombres y tipos distintos

* El esquema final en Redshift se romper√°, perdiendo trazabilidad y control del modelo de datos

* Los nuevos tipos de datos son compatibles finalmente para la tabla destino en Redshift

* ‚ö†Ô∏è Algunos campos quedar√°n en NULL por la naturaleza de los datos, esto permitir√° analizarlos en Redshift.

Con el nodo **Change Schema**, se transforman expl√≠citamente los tipos de datos del flujo para que coincidan exactamente con los definidos en la tabla destino de Redshift, asegurando una carga limpia y consistente.


#### ****üîåüîÑ Fase 3: Destino (Target)****

**vi) Configuraci√≥n del destino: Amazon Redshift (Target)**

Como **pen√∫ltimo paso** en la elaboraci√≥n del **Visual ETL** en **AWS Glue**, se procede a configurar el **nodo de destino (Target)** llamado **Amazon Redshift**.
Este nodo permitir√° cargar los datos transformados hacia un **Data Warehouse** previamente creado en **Amazon Redshift**, utilizando una conexi√≥n segura definida expl√≠citamente para Glue.

Antes de configurar este nodo dentro del Visual ETL, es necesario realizar los siguientes **sub-pasos previos**:

#### a) Creaci√≥n del Data Warehouse en Amazon Redshift Serverless

Amazon Redshift es un servicio serverless, donde AWS administra la mayor parte de la infraestructura, permiti√©ndonos enfocarnos √∫nicamente en el modelo de datos y las consultas.

##### **1. Creaci√≥n del Workgroup (Grupo de Trabajo)**

Desde el servicio **Amazon Redshift**, seleccionamos **Crear grupo de trabajo (Workgroup)** y configuramos √∫nicamente los campos m√°s relevantes:

* Nombre del grupo de trabajo: ```wk-brayan-project-s3-glue-redshift```

En el apartado **Redes y seguridad**, configuramos:

* IP address type: IPv4

* VPC: VPC por defecto de AWS

* Security Group: grupo de seguridad por defecto

* Subnets:

   Redshift requiere al menos 3 subredes para alta disponibilidad.
   La VPC por defecto ya incluye 3 subredes, por lo que se dejan seleccionadas autom√°ticamente.

Se dejan los dem√°s valores por defecto y se contin√∫a.

##### **2. Creaci√≥n del Namespace (credenciales)**

El **Namespace** define las credenciales y el acceso l√≥gico a la base de datos.

* Nombre del Namespace: ```ns-brayan-project-s3-glue-redshift```

* Se selecciona **Personalizar** credenciales del usuario administrador

* Seleccionamos **Agregar manualmente la contrase√±a del administrador**

* En **Contras√±a de usuario administrador** ingresamos manualmente (solo para pr√°cticas): ```12345Rafael#```

> ‚ö†Ô∏è En un entorno productivo, lo recomendado es permitir que AWS Secrets Manager genere y administre las credenciales.

##### **3. Rol de IAM para Redshift (acceso a S3)**

Aqu√≠ surge una duda com√∫n:
**¬øEste rol es el mismo que el rol de Glue?**

* üëâ No. Es un rol distinto.

Este rol permite que Redshift acceda directamente a S3 para leer y escribir datos.

Configuraci√≥n del rol:

* Access Type: **Full Access**

* S3 Buckets: se selecciona √∫nicamente el bucket del proyecto (``` project-s3-glue-redshift ```)

* Se crea el rol como predeterminado y se asigna al Namespace (rol creado a borrar despues AmazonRedshift-CommandsAccessRole-20260122T181821 )

Le damos a **Siguiente**, se revisan las configuraciones y se procede a **Crear**.

> ‚è≥ Se debe esperar a que el Workgroup y el Namespace se creen correctamente.
Una vez creados, aparecer√°n listados en el panel principal de Redshift.

#### b) Creaci√≥n de la conexi√≥n Glue ‚Üí Redshift (JDBC)

Este paso es cr√≠tico, debido que permite que AWS Glue pueda conectarse a Redshift para escribir los datos del ETL.

Existen **dos formas** de crear esta conexi√≥n:

‚ùå Opci√≥n 1: UI de Glue (NO recomendada)

Aunque se puede crear una conexi√≥n desde Glue ‚Üí Data Connections, esta opci√≥n suele generar errores, debido que:

* La conexi√≥n termina asoci√°ndose a la VPC

* Glue requiere una Subnet espec√≠fica, no solo la VPC

Esto provoca errores de validaci√≥n al ejecutar el job üòÖ

‚úÖ Opci√≥n 2: AWS CloudShell (RECOMENDADA)

Desde AWS CloudShell, se ejecuta el siguiente comando para crear la conexi√≥n correctamente:
```bash
aws glue create-connection \
  --connection-input '{
    "Name": "glue-redshift-connection",
    "ConnectionType": "JDBC",
    "Description": "Glue connection to Redshift Serverless",
    "ConnectionProperties": {
      "JDBC_CONNECTION_URL": "<URL_JDBC_DEL_WORKSPACE>",
      "USERNAME": "admin",
      "PASSWORD": "<PASSWORD_DEL_NAMESPACE>",
      "JDBC_ENFORCE_SSL": "true"
    },
    "PhysicalConnectionRequirements": {
      "SubnetId": "<SUBNET_DEL_WORKSPACE_ZONA_DISPONIBILIDAD_2a>",
      "SecurityGroupIdList": ["<SECURITY_GROUP_DEL_WORKSPACE>"],
      "AvailabilityZone": "us-east-2a"
    }
  }'
```
üìå Todos estos valores se obtienen desde los detalles del Workgroup en Redshift. Adem√°s, les recomiendo utilizar la zona de disponibilida **a**.

#### c) Creaci√≥n del VPC Endpoint para S3 (Gateway)

Para que Glue pueda leer desde S3 y escribir en Redshift **sin errores de red**, es necesario crear un **VPC Endpoint** de tipo **Gateway** para S3.

Pasos:

1. Ir a VPC ‚Üí PrivateLink y Lattice ‚Üí Endpoints

2. Crear un nuevo endpoint con:

   * Nombre: endpoint-glue-s3-project-glue-redshift

   * Tipo: Servicios de AWS

   * Servicio: ```com.amazonaws.us-east-2.s3``` (tipo Gateway)

   * VPC: VPC por defecto

   * Route Tables: tabla principal de la VPC

   * Policy: Full Access

Esto soluciona errores como:
```text
InvalidInputException - VPC S3 endpoint validation failed for SubnetId...
Could not find S3 endpoint or NAT gateway...
```
üìå Este error indica que la subnet privada no ten√≠a acceso a S3, lo cual es obligatorio para Glue.

#### d) Creaci√≥n de la tabla destino en Redshift

Con Redshift, la conexi√≥n y la red correctamente configuradas, se procede a crear la tabla destino.

1. Acceder a **Datos de consulta** en Amazon Redshift

2. Conectarse al Workgroup usando **Federated User** (Lado izquierdo aparecera un men√∫ con **Serverless: nombre_workspace_definido**).
   Despu√©s, click derecho y **Create connection**

3. Ubicaci√≥n del esquema:
```java
Workgroup
  ‚îî‚îÄ‚îÄ native databases
      ‚îî‚îÄ‚îÄ dev
          ‚îî‚îÄ‚îÄ public
```
Query para crear la tabla destino:
```sql
CREATE TABLE sales
(
    order_id INT,
    order_date DATE,
    customer TEXT,
    product TEXT,
    category TEXT,
    quantity INT,
    unit_price DECIMAL(10,2),
    region TEXT,
    total_sale DECIMAL(10,2)
);
```
Se ejecuta la consulta y la tabla queda creada correctamente.

üìå Es fundamental que el esquema coincida exactamente con el esquema final definido en Glue (especialmente despu√©s del nodo **Change Schema**).

**VII. Configuraci√≥n del nodo Amazon Redshift y ejecuci√≥n del ETL**

Ahora bien, una vez realizada la configuraci√≥n previa en **Amazon Redshift**, tales como la creaci√≥n del **Workspace**, **Namespace**, **la conexi√≥n entre AWS Glue y Redshift**, as√≠ como la configuraci√≥n del **endpoint dentro de la VPC**, procederemos a configurar el nodo elegido en el paso VI, denominado **‚ÄúAmazon Redshift‚Äù**, el cual se encuentra dentro de los nodos de tipo Target.

En el apartado de configuraci√≥n del nodo, espec√≠ficamente en **‚ÄúRedshift access type‚Äù**, seleccionaremos la opci√≥n recomendada por AWS:

* **Direct data connection ‚Äì recommended**

En el siguiente apartado, **‚ÄúRedshift connection‚Äù**, elegiremos el conector previamente creado entre Glue y Redshift, el cual en este caso se denomina:

* ```glue-redshift-connection```

Una vez seleccionado el tipo de acceso y el conector de Redshift, procederemos a elegir **el esquema por defecto** que nos brinda la base de datos / Data Warehouse de Redshift:

* Schema: ```public```

Posteriormente, seleccionaremos la tabla creada anteriormente:

* Tabla destino: ```sales```

Adicionalmente, en la primera ejecuci√≥n del proceso ETL, el apartado **‚ÄúHandling of data and target data‚Äù** se encontrar√° configurado por defecto en **APPEND**, lo que indica que cada vez que se ejecute el ETL se insertar√°n todos los datos, existan o no previamente, permitiendo duplicados.

Sin embargo, la opci√≥n recomendada es **MERGE**, debido que:

* Evita la inserci√≥n de registros duplicados

* Actualiza la informaci√≥n existente

* Funciona como un **UPSERT (UPDATE + INSERT)**

* Luego, por defecto se seleccionarpa **Choose keys and simple actions**, donde elegiremos en **Matching keys** la columna que representar√° la clave primaria
  en la tabla **sales**, en este caso, se elige la columna **order_id**.

Finalmente, guardamos la configuraci√≥n del **Visual ETL**, seleccionamos **Run** y se iniciar√° el proceso ETL. El estado de la ejecuci√≥n puede verificarse en el apartado **‚ÄúRuns‚Äù**, donde el estado cambiar√° de **Running** a **Succeeded**.

Para validar que el proceso ETL se ejecut√≥ correctamente, nos dirigimos al **Editor de consultas SQL de Redshift** y ejecutamos la siguiente consulta:
```sql
SELECT * FROM sales
```
Si los datos se muestran correctamente, se confirma que el proceso ETL se ejecut√≥ de manera exitosa y que la informaci√≥n se encuentra almacenada en Amazon Redshift.
